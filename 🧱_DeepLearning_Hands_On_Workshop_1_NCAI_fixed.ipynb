{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# \ud83e\udded **1. Introduction**\n",
        "\n",
        "---\n",
        "\n",
        "## \ud83d\udc4b **1.1 About Me:**\n",
        "\n",
        "### \ud83c\udf99\ufe0f Speaker Introduction\n",
        "\n",
        "Welcome!  \n",
        "\n",
        "\n",
        "**\ud83d\udc68\u200d\ud83c\udfeb Usama Arshad**\n",
        "- Assistant Professor (Business Analytics), FAST National University, Islamabad  \n",
        "- PhD in Computer Science \u2013 Blockchain & AI, Ghulam Ishaq Khan Institute  \n",
        "- Research Assistant \u2013 National Yunlin University, Taiwan (AI in Healthcare)  \n",
        "- President \u2013 Graduate Students Society, GIKI  \n",
        "- Published Author \u2013 IEEE, Springer, Elsevier (Blockchain, AI, Cybersecurity)  \n",
        "- GitHub: [github.com/usamajanjua9](https://github.com/usamajanjua9)  \n",
        "- LinkedIn: [linkedin.com/in/usamajanjua9](https://linkedin.com/in/usamajanjua9)  \n",
        "- Website: [usamajanjua.com](https://usamajanjua.com)\n",
        "\n",
        "<img src=\"https://isb.nu.edu.pk/Images/Profile/FSM/7078.jpg\" width=\"350\" >\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "bfGQGc3hD20g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Day 1 \u2013 Deep Learning & Neural Networks Basics**\n",
        "\n",
        "## **1. Introduction to Deep Learning**\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd39 **1.1 Definition**\n",
        "\n",
        "* **Deep Learning (DL)** is a branch of **Artificial Intelligence (AI)** and a subset of **Machine Learning (ML)**.\n",
        "* It uses **artificial neural networks with many hidden layers** to learn patterns automatically.\n",
        "* The word *\u201cdeep\u201d* refers to the presence of **multiple layers** in the network.\n",
        "* These models are highly effective for **large, unstructured data** (\ud83d\udcf8 images, \ud83c\udfa4 audio, \ud83d\udcc4 text).\n",
        "\n",
        "**\ud83d\udc49 In simple terms:** Deep Learning = Neural Networks with many layers + big data + powerful computation = \ud83d\ude80 advanced learning power.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd39 **1.2 Core Characteristics**\n",
        "\n",
        "\u2728 Some unique traits of Deep Learning:\n",
        "\n",
        "1. \ud83e\udde9 **Hierarchical Learning** \u2013 Learns from simple patterns \u2192 to complex representations.\n",
        "2. \u26a1 **End-to-End Training** \u2013 Directly learns from raw input \u2192 output.\n",
        "3. \ud83d\udcca **Scalable** \u2013 Performs better with more data & GPUs.\n",
        "4. \ud83d\udd12 **Generalization** \u2013 Works across multiple domains (healthcare, finance, vision, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd39 **1.3 Examples of Deep Learning**\n",
        "\n",
        "- \ud83d\udcf8 **Computer Vision** \u2192 Face unlock, medical imaging, self-driving cars.\n",
        "- \ud83c\udfa4 **Speech Recognition** \u2192 Siri, Alexa, Google Assistant.\n",
        "- \ud83d\udcac **NLP (Language)** \u2192 Google Translate, ChatGPT, chatbots.\n",
        "- \ud83d\udcb3 **Finance** \u2192 Fraud detection, stock prediction.\n",
        "- \ud83c\udfac **Recommendations** \u2192 Netflix, YouTube, Amazon.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd39 **1.4 Benefits**\n",
        "\n",
        "- \u2705 **High Accuracy** \u2013 Better than traditional ML in vision/NLP.\n",
        "- \u2705 **Feature Automation** \u2013 No need for manual feature engineering.\n",
        "- \u2705 **Versatility** \u2013 Works on text, images, speech, time-series.\n",
        "- \u2705 **Continuous Learning** \u2013 Improves as data grows.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd39 **1.5 Limitations**\n",
        "\n",
        "- \u26a0\ufe0f Needs **large labeled datasets**.\n",
        "- \u26a0\ufe0f Requires **high computational power (GPUs/TPUs)**.\n",
        "- \u26a0\ufe0f Often a **black box** \u2013 difficult to interpret decisions.\n",
        "- \u26a0\ufe0f Risk of **overfitting** if not handled carefully.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd39 **1.6 Real-World Use Cases**\n",
        "\n",
        "- \ud83c\udfe5 **Healthcare** \u2013 Tumor detection, drug discovery.\n",
        "- \ud83c\udfe6 **Finance** \u2013 Fraud detection, credit scoring.\n",
        "- \ud83d\uded2 **Retail** \u2013 Personalized recommendations.\n",
        "- \ud83d\ude97 **Transportation** \u2013 Self-driving cars.\n",
        "- \ud83d\udcf1 **Social Media** \u2013 Content moderation, caption generation.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udfaf **Mini Student Activity**\n",
        "\n",
        "\n",
        "\ud83d\udc49 *\u201cWhere do you see Deep Learning in your daily life? Give one example.\u201d*\n",
        "(e.g., YouTube recommending videos, Face ID, Google Translate, etc.)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PhJPEQV2EwqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **1.2 Difference between Machine Learning (ML) & Deep Learning (DL)**\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcd6 **Definition Recap**\n",
        "\n",
        "* **Machine Learning (ML):**\n",
        "  Algorithms learn patterns from data and make predictions. Usually requires **manual feature extraction**.\n",
        "* **Deep Learning (DL):**\n",
        "  A subset of ML that uses **multi-layer neural networks** to automatically extract features and learn complex patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcca **Comparison Table**\n",
        "\n",
        "| \ud83d\udd0e Feature              | \ud83e\udd16 Machine Learning (ML)                                      | \ud83e\udde0 Deep Learning (DL)                                    |\n",
        "| ----------------------- | ------------------------------------------------------------- | -------------------------------------------------------- |\n",
        "| **Feature Engineering** | Requires **manual feature extraction** by experts             | **Automatic feature extraction** from raw data           |\n",
        "| **Data Requirements**   | Works well on **small to medium datasets**                    | Needs **large datasets** to perform well                 |\n",
        "| **Computation**         | Runs on CPU, less resource-intensive                          | Requires **GPUs/TPUs**, high computational power         |\n",
        "| **Execution Time**      | Faster training on small data                                 | Slower training, but more accurate                       |\n",
        "| **Accuracy**            | Good for simple/moderate tasks                                | State-of-the-art for complex tasks (vision, speech, NLP) |\n",
        "| **Interpretability**    | Easier to understand (transparent models like Decision Trees) | Often a **black box**, harder to interpret               |\n",
        "| **Examples**            | Spam email filter, loan approval, stock prediction            | Self-driving cars, voice assistants, medical imaging     |\n",
        "\n",
        "---\n",
        "\n",
        "### \u2705 **Key Takeaways**\n",
        "\n",
        "* ML is like **traditional learning** \u2192 requires **human-designed features**.\n",
        "* DL is like **end-to-end learning** \u2192 feeds raw data \u2192 network learns features automatically.\n",
        "* DL outperforms ML when **data is huge** and **problem is complex**.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udf0d **Examples**\n",
        "\n",
        "* **Machine Learning (ML):**\n",
        "\n",
        "  * Predicting house prices (using size, location, rooms).\n",
        "  * Spam email detection.\n",
        "  * Simple fraud detection.\n",
        "\n",
        "* **Deep Learning (DL):**\n",
        "\n",
        "  * Google Translate (language understanding).\n",
        "  * Facial recognition in iPhones.\n",
        "  * Detecting diseases from X-rays/MRIs.\n",
        "  * Autonomous driving (Tesla, Waymo).\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udfaf **Mini Activity for Students**\n",
        "\n",
        "Ask:\n",
        "\ud83d\udc49 *\u201cIf you had only 500 labeled images of cats and dogs, which approach would you use \u2013 ML or DL? Why?\u201d*\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "r9t_wOgDFL_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **1.3 Real-World Applications of Deep Learning**\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udfe5 **Healthcare**\n",
        "\n",
        "* **Medical Imaging:** Detecting tumors, fractures, or abnormalities from X-rays, CT scans, MRI.\n",
        "* **Drug Discovery:** Predicting molecule behavior to design new medicines faster.\n",
        "* **Patient Monitoring:** Wearable devices using DL for real-time health alerts.\n",
        "\n",
        "\ud83d\udc49 *Example:* Deep Learning models achieve dermatologist-level accuracy in detecting skin cancer.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\ude97 **Autonomous Vehicles**\n",
        "\n",
        "* **Object Detection:** Identifying pedestrians, vehicles, and traffic signals.\n",
        "* **Lane Detection:** Assisting with lane-keeping in self-driving cars.\n",
        "* **Decision Making:** Combining sensors + DL to navigate safely.\n",
        "\n",
        "\ud83d\udc49 *Example:* Tesla & Waymo use CNNs for real-time vision systems.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcb3 **Finance**\n",
        "\n",
        "* **Fraud Detection:** Identifying unusual transaction patterns.\n",
        "* **Algorithmic Trading:** Predicting market trends for high-frequency trading.\n",
        "* **Credit Scoring:** More accurate risk assessment compared to traditional methods.\n",
        "\n",
        "\ud83d\udc49 *Example:* Banks deploy DL to flag fraudulent credit card transactions instantly.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\uded2 **Retail & E-commerce**\n",
        "\n",
        "* **Recommendation Systems:** Personalized product suggestions (Amazon, Alibaba).\n",
        "* **Customer Sentiment Analysis:** Understanding reviews and feedback automatically.\n",
        "* **Inventory Optimization:** Forecasting demand to reduce wastage.\n",
        "\n",
        "\ud83d\udc49 *Example:* Netflix uses DL to recommend shows/movies that align with user preferences.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udcf1 **Social Media & Communication**\n",
        "\n",
        "* **Content Moderation:** Filtering harmful or inappropriate posts.\n",
        "* **Automatic Captioning:** Generating captions for images & videos.\n",
        "* **Chatbots & Virtual Assistants:** Handling customer queries.\n",
        "\n",
        "\ud83d\udc49 *Example:* Facebook uses DL to detect hate speech in multiple languages.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udef0\ufe0f **Other Domains**\n",
        "\n",
        "* \ud83c\udf26\ufe0f **Weather Forecasting & Climate Modeling**\n",
        "* \ud83d\udef0\ufe0f **Satellite Image Analysis (Agriculture, Disaster Management)**\n",
        "* \ud83c\udfae **Gaming (AI bots, realistic graphics generation)**\n",
        "* \ud83c\udfa8 **Art & Creativity (AI-generated paintings, music, videos)**\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udfaf **Quick Student Activity**\n",
        "\n",
        "\n",
        "\ud83d\udc49 *\u201cCan you name one app or service you personally use that is powered by Deep Learning?\u201d*\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2hVoavuDHJaw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **2. Neural Network Fundamentals**\n",
        "\n",
        "---\n",
        "\n",
        "## **2.1 What is a Neuron?**\n",
        "\n",
        "* Inspired by the **biological neuron**, but purely mathematical.\n",
        "* A neuron takes inputs \u2192 multiplies them by weights \u2192 adds bias \u2192 passes result through an **activation function** \u2192 outputs a value.\n",
        "\n",
        "**Formula:**\n",
        "$$\n",
        "[\n",
        "y = f\\left(\\sum (w_i \\cdot x_i) + b \\right)\n",
        "]\n",
        "$$\n",
        "Where:\n",
        "\n",
        "* $$( x_i ) = inputs$$\n",
        "* $$( w_i ) = weights$$\n",
        "* $$( b ) = bias$$\n",
        "* $$( f ) = activation function (ReLU, Sigmoid, etc.)\n",
        "$$\n",
        "\ud83d\udc49 Each neuron is a **decision unit** that learns patterns during training.\n",
        "\n",
        "---\n",
        "\n",
        "## **2.2 Layers in a Neural Network**\n",
        "\n",
        "1. **Input Layer** \ud83c\udfaf\n",
        "\n",
        "   * Receives raw data (e.g., pixel values of an image, numerical features of a dataset).\n",
        "2. **Hidden Layers** \ud83d\udd04\n",
        "\n",
        "   * Multiple layers where neurons learn progressively **complex features**.\n",
        "   * More layers = deeper network.\n",
        "3. **Output Layer** \ud83d\udcca\n",
        "\n",
        "   * Produces the final prediction (e.g., class probabilities for cat/dog).\n",
        "\n",
        "---\n",
        "\n",
        "## **2.3 Network Architecture**\n",
        "\n",
        "* **Feedforward Neural Network (FNN):** Data flows in one direction from input \u2192 output.\n",
        "* **Number of Layers:** Determines if it\u2019s a **shallow** or **deep** network.\n",
        "* **Parameters:**\n",
        "\n",
        "  * **Weights:** Strength of connection between neurons.\n",
        "  * **Bias:** Shifts the activation to improve learning.\n",
        "  * **Activation Functions:** Decide whether a neuron \u201cfires\u201d or not.\n",
        "\n",
        "---\n",
        "\n",
        "## **2.4 Activation Functions**\n",
        "\n",
        "* **Sigmoid (\u03c3):** Outputs values between 0 and 1 \u2192 good for probabilities.\n",
        "* **ReLU (Rectified Linear Unit):** Fast, avoids vanishing gradients, most commonly used.\n",
        "* **Softmax:** Used in the output layer for multi-class classification.\n",
        "\n",
        "\ud83d\udc49 Choosing the right activation function is critical for learning efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "## **2.5 Example**\n",
        "\n",
        "* Input: Image of a handwritten digit \"5\"\n",
        "* Layers:\n",
        "\n",
        "  * **Input Layer:** Pixel values (28\u00d728 = 784 inputs).\n",
        "  * **Hidden Layers:** Detect edges, shapes, patterns.\n",
        "  * **Output Layer:** Predicts class (0\u20139).\n",
        "\n",
        "---\n",
        "\n",
        "## **2.6 Why Layers Matter**\n",
        "\n",
        "* **First layers:** Learn low-level features (edges, colors).\n",
        "* **Middle layers:** Learn higher-level shapes and patterns.\n",
        "* **Last layers:** Learn task-specific features (digits, faces, objects).\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udfaf **Mini Student Activity**\n",
        "\n",
        "* Draw a **simple 3-layer neural network** (Input \u2192 Hidden \u2192 Output).\n",
        "* Label: Inputs, weights, biases, and activation function.\n",
        "* Ask students: *\u201cWhat happens if we remove the hidden layer?\u201d*\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "hNFYohOvQl5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **2.4 Loss Functions & Backpropagation**\n",
        "\n",
        "---\n",
        "\n",
        "## **Loss Functions**\n",
        "\n",
        "### \ud83d\udd39 **Definition**\n",
        "\n",
        "* A **loss function** (or cost function) measures **how well** the neural network\u2019s predictions match the actual outputs.\n",
        "* It\u2019s the **error signal** that tells the network *\u201chow wrong it is.\u201d*\n",
        "* During training, the goal is to **minimize the loss**.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd39 **Types of Loss Functions**\n",
        "\n",
        "1. \ud83d\udcc9 **Mean Squared Error (MSE)** \u2013 used for regression tasks.\n",
        "   $$[\n",
        "   L = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
        "   ]$$\n",
        "\n",
        "2. \ud83d\udcca **Cross-Entropy Loss** \u2013 used for classification tasks.\n",
        "   $$[\n",
        "   L = - \\sum y_i \\cdot \\log(\\hat{y}_i)\n",
        "   ]$$\n",
        "\n",
        "3. \u2696\ufe0f **Hinge Loss** \u2013 used for SVMs and binary classification.\n",
        "\n",
        "\ud83d\udc49 **Which one to use?**\n",
        "\n",
        "* Regression \u2192 MSE\n",
        "* Binary classification \u2192 Binary Cross-Entropy\n",
        "* Multi-class classification \u2192 Categorical Cross-Entropy\n",
        "\n",
        "---\n",
        "\n",
        "## **Backpropagation**\n",
        "\n",
        "### \ud83d\udd39 **Definition**\n",
        "\n",
        "* Backpropagation is the algorithm used to **train neural networks**.\n",
        "* It works by adjusting the **weights and biases** of neurons using the error from the loss function.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd39 **Steps in Backpropagation**\n",
        "\n",
        "1. **Forward Pass**\n",
        "\n",
        "   * Input flows through the network \u2192 produces output.\n",
        "\n",
        "2. **Loss Calculation**\n",
        "\n",
        "   * Compare prediction ((\\hat{y})) with true label ((y)) using a loss function.\n",
        "\n",
        "3. **Backward Pass (Backpropagation)**\n",
        "\n",
        "   * Compute gradients (partial derivatives) of loss w.r.t. weights using the **chain rule of calculus**.\n",
        "\n",
        "4. **Weight Update**\n",
        "\n",
        "   * Update weights using **Gradient Descent**:\n",
        "     [\n",
        "     w = w - \\eta \\cdot \\frac{\\partial L}{\\partial w}\n",
        "     ]\n",
        "     where (\\eta) = learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83d\udd39 **Optimization Algorithms**\n",
        "\n",
        "* **Stochastic Gradient Descent (SGD)**\n",
        "* **Adam Optimizer** (commonly used, adapts learning rate)\n",
        "* **RMSprop, Adagrad** (specialized optimizers)\n",
        "\n",
        "---\n",
        "\n",
        "## **Example**\n",
        "\n",
        "* Task: Classify whether an image is a **cat (1)** or **dog (0)**.\n",
        "* Prediction = 0.7 (cat probability).\n",
        "* True label = 1 (cat).\n",
        "* Loss function \u2192 computes error = 0.36.\n",
        "* Backpropagation \u2192 reduces error by adjusting weights until prediction gets closer to 1.\n",
        "\n",
        "---\n",
        "\n",
        "## **Why It Matters**\n",
        "\n",
        "* Without loss functions, the network doesn\u2019t know *how wrong it is*.\n",
        "* Without backpropagation, the network can\u2019t learn from mistakes.\n",
        "\n",
        "---\n",
        "\n",
        "### \ud83c\udfaf **Mini Student Activity**\n",
        "\n",
        "Ask:\n",
        "\ud83d\udc49 *\u201cWhy can\u2019t we just set weights randomly and skip backpropagation?\u201d*\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "XdBBSsj-S8Gn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **3. Building a Neural Network in Python**\n",
        "\n",
        "We\u2019ll use **TensorFlow & Keras** (student-friendly, high-level API).\n",
        "Dataset: **MNIST Handwritten Digits (0\u20139)** \ud83d\udd8a\ufe0f\n",
        "\n",
        "---\n",
        "\n",
        "## **3.1 Setup**\n",
        "\n",
        "```python\n",
        "# Install (if needed) in Colab\n",
        "!pip install tensorflow\n",
        "```\n",
        "\n",
        "```python\n",
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **3.2 Load Dataset**\n",
        "\n",
        "```python\n",
        "# Load MNIST data (images + labels)\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize (scale pixel values 0-255 \u2192 0-1)\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# One-hot encode labels (e.g., 5 \u2192 [0,0,0,0,0,1,0,0,0,0])\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "print(\"Training data shape:\", x_train.shape)\n",
        "print(\"Test data shape:\", x_test.shape)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **3.3 Build Model**\n",
        "\n",
        "```python\n",
        "# Define a simple Neural Network\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),     # Flatten 28x28 image \u2192 784 vector\n",
        "    Dense(128, activation='relu'),     # Hidden layer with 128 neurons\n",
        "    Dense(64, activation='relu'),      # Another hidden layer\n",
        "    Dense(10, activation='softmax')    # Output layer (10 classes)\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **3.4 Train Model**\n",
        "\n",
        "```python\n",
        "# Train for 5 epochs\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=5,\n",
        "                    validation_data=(x_test, y_test))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **3.5 Evaluate Model**\n",
        "\n",
        "```python\n",
        "# Evaluate accuracy on test set\n",
        "loss, acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {acc:.4f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **3.6 Make Predictions**\n",
        "\n",
        "```python\n",
        "# Predict first 5 test images\n",
        "import numpy as np\n",
        "\n",
        "predictions = model.predict(x_test[:5])\n",
        "print(\"Predicted labels:\", np.argmax(predictions, axis=1))\n",
        "print(\"True labels:     \", np.argmax(y_test[:5], axis=1))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Expected Outcome**\n",
        "\n",
        "* Accuracy on MNIST test set: ~97\u201398% \ud83c\udfaf\n",
        "* Students see how just a few lines of code \u2192 powerful model.\n",
        "\n",
        "---\n",
        "\n",
        "## **Interactive Questions for Students**\n",
        "\n",
        "1. What happens if we **remove one hidden layer**?\n",
        "2. What if we **increase epochs from 5 \u2192 20**?\n",
        "3. Why do we use **softmax** in the last layer?\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "p332p4jkTYWz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e3953e7501c544e6bc83c59e234d625d",
            "7b27e3397140481fa050844d6ff3be37",
            "10063b4e42a84edaa9d80ca720cce1a1",
            "6579979cfaf84d0384112ea7a818377d",
            "0ae478c68f404257bc8c3527638cd7b1",
            "eb947384a54b4cff83a50e0cafa3eed0",
            "caa917a355dc4a148e68d1113e1ca69c",
            "31e95ca9cfe74af9874629ce17fffb92",
            "074e440dbe9e4003a2f60bb8bc60bdbe",
            "1e8c523744f547cf815a23501c475715",
            "db73052dce2b4069ae08bc599db5d09f",
            "57c06b0eab574289b4bd3ba8068b26c5",
            "6917ec48495340debbc281d1e14c7f02",
            "2316036e5f474bf5826e4678d9c2f0ff",
            "b3703361f5f345e0a09ec277af992315",
            "87b45857ebb14acb855e6fc4df66585b",
            "85d2487148eb4e51ae49d21e176f37c0",
            "5b3cb015306349ed8c97c4a1c692fe23",
            "42b6b7cb4f534cb2bc72d0a3e4599cb1",
            "b150f12a98324a4d973e8d4733493f17",
            "792c646d5e7849f197b9e6b61cf19989",
            "071e1f5af9374946ac7e32b3d5379324",
            "95ce4215253847359f895dbafcc17bef",
            "9cb872a39c3c458a8c5115a20d866f79",
            "c0c6b8e518ee4eb391da0e4316e4b68b",
            "8f317386bed14e1c8fd0926625139551",
            "cea2095e8e444210a6da9045bad7a932",
            "75ad8f3b04d742ae96e6d622a50812b8",
            "e50aeb5ef7274101923347e1b024b7aa",
            "4fbb3dab19904d9498771285f9f4f7fe",
            "c0e366318fa146a1a3d6351f344f0204",
            "ec16b99be60e4148b4994c9076ffc7cb",
            "3a41c632ce0e485791d12f0d0af7205b",
            "a90bb2cc694a48b4861a00d100d141f3",
            "b4611f686e0347e7988d117e705816a3",
            "f001ea66c2484ddb989f947ca8e4bd5b",
            "236819e9656b4ada813a2684a404002a",
            "2725d2fc867e4fcfa7b6619e9d7137b1",
            "f373247d501c4326bbfd7977fff4e68b",
            "d89fb68ac7ff461ba77bee0f9232cf28",
            "cecceed470c941a88309616b29f9eee3",
            "03ad56c1a0214d8487e81922100c2a3d",
            "1d67b11067df4a469da75024e88b7076",
            "f9b48ac28194461286d09dda577ff08b",
            "5c31d158795e49b69379485bc491cd84",
            "b06ad6e2097a488d8d6c3cce39ed00bb",
            "9f73ef943d2a4ab38979bb945b1248f4",
            "1c0c40c8aafc49078419d324250c0f4c",
            "70a0887e9c554b89861e9a73d9d6f1d1",
            "c435e1c23aeb4fd18caf867cd5873e29",
            "f2418f13e515422a87e726d6bf71989d",
            "181ea064d07f44d5bc8a81e2ef3b164e",
            "072aacc3dcb94a6eb219d5a065be73c2",
            "1d6231f010ba4e9b9f86ac771fda2b76",
            "a7124e79bf974171b7d5bc11051a9a15",
            "e4bcfb48ecc44ac69f28978d37e40cc2",
            "dd060d47dffa42e9ab2f32099752b3bd",
            "aa0b19d3fb184612941a1f57e9025302",
            "0030f2ba5e7f4b6e99b19493e70431df",
            "87d2840092f94bc49a416df2955c7830",
            "77d02d66c49442fab3db5e9724a64b68",
            "8a851a4dcca54ce2bfadbb9d558fb675",
            "7340a771b6b84041a4e7d64336c422e1",
            "38927a762ce348f1b74dca805afd5305",
            "ae200b528ac1467191c3e12126f2cd4d",
            "bed958fce4a94456aede5a75b81f759c",
            "d0c67acdbaa64e9dbfaa5975a84afac5",
            "edc42d4cdcba4102805ab8fd2b8d96ef",
            "848a1d582048411a99442f4b7a8d4757",
            "8589ac3f13744ca6826cec910a10f022",
            "2b43e463360f43d0b45a9a63b6a1c22b",
            "5abe8ce46f2b455d9c36729419e6c74e",
            "86bf305d54d24230bdc35301425ddc5d",
            "42173fef1685467194d742c467d107f0",
            "79774fc4fbc046c8b014aa64e52e20cf",
            "5ae88f32a370428787fb94249596b2a3",
            "881ec6a699a54295b3f4e0d9d8fe0b75",
            "e722a79cdbc64975a5b7ce28bbf8ed49",
            "2063a62b0fa0421a985e90844f143fcc",
            "ce44ead95ace45b4b2e0a1b2ece6d087",
            "a4ad06bf55324d7081dd37b80fb1d492",
            "3462159b05f9495297fcd603cd3e1368",
            "199fd99f0bb746cf931cba426aa13160",
            "570105b2f0954a99bac7d2d73187f89a",
            "caa31bee90064bbea2e3d2f053cf1e77",
            "b63be0b26486442089f6c178ac4f5df8",
            "7a945ecaaa094b71a12d963ecdab8e6e",
            "35558495782345808d68715362f16c02",
            "5bd0cda50b0d4f048126fc03500888e4",
            "363b5951c93e4143b353ae7d961a692d",
            "2dcbbd98f056459d992cb3c0643938bc",
            "62e3ee1fc9a547f381738f7fb09aeabe",
            "c5d5f08e36574d1ebc859bc7bae0d377",
            "13256230d39043489fcb0d654caa7344",
            "8330dbdd5a0b40a1a482e57f840c63e4",
            "aca7f0bfea3640a9b8d722e0ca768961",
            "dc30c91ebb92438b88ff06d9cb243c76",
            "1765a5da4f2046e68c0d2849924be8cd",
            "32503a227bea43209dcc34894f71c7b6",
            "04f0e49fd5354e2a8ffc8ce84a770b30",
            "c4678568ee8d4f178eed6652e57591de",
            "6aaee58136c645fda44bf162fe8af8bf",
            "0e6c1eb9257946e9a333a8538043bb28",
            "d7a8d318c7e74b6985dbc052415d3a2d",
            "a2e91a5ce57148f7945b682c267d4882",
            "e61d88232907411e8e2e86036140234d",
            "df8d66a82ca1438296b3b429ff419b66",
            "5c238e223735429c9d5011ecd9894555",
            "963d5b73895f4d9e924db7621b52225d",
            "3fbf05897d9c478b8aa59d7d5d1400bc",
            "d5aa590f2e3548a29e3297422cbc2084",
            "986d045b935542f3a5a8cdd3823a5a37",
            "2b250d93e4e542f3bd7f991ad4ac8010",
            "e1a8fb74a39b49c5bdbf9b1b8ee93c08",
            "9100ecd0ded64100ac009feeb7aa1a8e",
            "2366871a892947c2a90425c764c93f8b",
            "10ef597b5c374c559ae10feae6df382d",
            "56221984faba42f2bb81c0a6bceb15d3",
            "011d71817cf8417b927895000fdca2f2",
            "5953ad21dc1f4539b2ab958b60ec87f3",
            "8f2db7c363484af8882fe0f54c900622"
          ]
        },
        "id": "clxy9fq1DuqI",
        "outputId": "380a1d25-6d63-4595-f8df-5175390af614"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HBox(children=(Dropdown(description='Dataset:', options=('MNIST', 'Fashion-MNIST', 'CIFAR-10'),\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e3953e7501c544e6bc83c59e234d625d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7576 - loss: 0.6920 - val_accuracy: 0.8455 - val_loss: 0.4192\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8570 - loss: 0.3938 - val_accuracy: 0.8718 - val_loss: 0.3556\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.8735 - loss: 0.3469 - val_accuracy: 0.8788 - val_loss: 0.3419\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8823 - loss: 0.3193 - val_accuracy: 0.8827 - val_loss: 0.3252\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.8888 - loss: 0.2985 - val_accuracy: 0.8852 - val_loss: 0.3199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "# ================================================\n",
        "# Deep Learning Interactive Demo (ipywidgets)\n",
        "# Author: Dr. Usama Arshad\n",
        "# ================================================\n",
        "# Usage:\n",
        "# 1) In Jupyter/Colab, run: pip install tensorflow ipywidgets scikit-learn matplotlib\n",
        "# 2) Run this script (or paste into a notebook cell) to get the interactive UI.\n",
        "# -----------------------------------------------\n",
        "\n",
        "# ========== Step 0: Imports & Setup ==========\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "from ipywidgets import (\n",
        "    Dropdown, ToggleButtons, IntSlider, FloatSlider, Checkbox, Text, IntText,\n",
        "    Button, VBox, HBox, Output, Label, Accordion, Layout\n",
        ")\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Make TF quieter\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
        "\n",
        "# Global refs\n",
        "GLOBAL_STATE = {\n",
        "    \"data\": None,\n",
        "    \"model\": None,\n",
        "    \"history\": None,\n",
        "    \"class_names\": None,\n",
        "    \"input_shape\": None,\n",
        "    \"num_classes\": None,\n",
        "    \"x_train\": None,\n",
        "    \"y_train\": None,\n",
        "    \"x_val\": None,\n",
        "    \"y_val\": None,\n",
        "    \"x_test\": None,\n",
        "    \"y_test\": None,\n",
        "}\n",
        "\n",
        "# ========== Step 1: Reproducibility Helper ==========\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "# ========== Step 2: Data Loading ==========\n",
        "def load_dataset(name=\"MNIST\", test_size=0.2, val_size=0.1, seed=42, normalize=True):\n",
        "    if name == \"MNIST\":\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "        class_names = [str(i) for i in range(10)]\n",
        "        x_train = x_train[..., np.newaxis]\n",
        "        x_test = x_test[..., np.newaxis]\n",
        "    elif name == \"Fashion-MNIST\":\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "        class_names = [\n",
        "            \"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\n",
        "            \"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"\n",
        "        ]\n",
        "        x_train = x_train[..., np.newaxis]\n",
        "        x_test = x_test[..., np.newaxis]\n",
        "    elif name == \"CIFAR-10\":\n",
        "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "        y_train = y_train.reshape(-1)\n",
        "        y_test = y_test.reshape(-1)\n",
        "        class_names = [\n",
        "            \"airplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\n",
        "            \"dog\",\"frog\",\"horse\",\"ship\",\"truck\"\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported dataset\")\n",
        "\n",
        "    if normalize:\n",
        "        x_train = x_train.astype(\"float32\") / 255.0\n",
        "        x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "    x_train, x_val, y_train, y_val = train_test_split(\n",
        "        x_train, y_train, test_size=val_size, random_state=seed, stratify=y_train\n",
        "    )\n",
        "\n",
        "    num_classes = len(np.unique(y_train))\n",
        "    input_shape = x_train.shape[1:]\n",
        "\n",
        "    y_train_cat = to_categorical(y_train, num_classes)\n",
        "    y_val_cat = to_categorical(y_val, num_classes)\n",
        "    y_test_cat = to_categorical(y_test, num_classes)\n",
        "\n",
        "    GLOBAL_STATE.update(dict(\n",
        "        class_names=class_names,\n",
        "        input_shape=input_shape,\n",
        "        num_classes=num_classes,\n",
        "        x_train=x_train, y_train=y_train_cat,\n",
        "        x_val=x_val, y_val=y_val_cat,\n",
        "        x_test=x_test, y_test=y_test_cat\n",
        "    ))\n",
        "\n",
        "    return (x_train, y_train_cat), (x_val, y_val_cat), (x_test, y_test_cat), num_classes, input_shape, class_names\n",
        "\n",
        "# ========== Step 3: Model Builders ==========\n",
        "def make_optimizer(name, lr):\n",
        "    name = name.lower()\n",
        "    if name == \"adam\":\n",
        "        return tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    if name == \"sgd\":\n",
        "        return tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
        "    if name == \"rmsprop\":\n",
        "        return tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
        "    raise ValueError(\"Unknown optimizer\")\n",
        "\n",
        "def build_mlp(input_shape, num_classes, hidden_units=(128,64), activation=\"relu\",\n",
        "              dropout=0.0, batchnorm=False, lr=1e-3, optimizer_name=\"adam\"):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "    model.add(layers.Flatten())\n",
        "    for units in hidden_units:\n",
        "        model.add(layers.Dense(int(units), activation=None))\n",
        "        if batchnorm:\n",
        "            model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Activation(activation))\n",
        "        if dropout and dropout > 0.0:\n",
        "            model.add(layers.Dropout(dropout))\n",
        "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "    model.compile(\n",
        "        optimizer=make_optimizer(optimizer_name, lr),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def build_cnn(input_shape, num_classes, conv_filters=(32,64), kernel_size=3,\n",
        "              pool_type=\"Max\", dense_units=128, activation=\"relu\", dropout=0.0,\n",
        "              batchnorm=False, lr=1e-3, optimizer_name=\"adam\", augment=False):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Input(shape=input_shape))\n",
        "\n",
        "    if augment:\n",
        "        model.add(layers.RandomFlip(\"horizontal\"))\n",
        "        model.add(layers.RandomRotation(0.1))\n",
        "        model.add(layers.RandomZoom(0.1))\n",
        "\n",
        "    for f in conv_filters:\n",
        "        model.add(layers.Conv2D(int(f), (kernel_size, kernel_size), padding=\"same\", activation=None))\n",
        "        if batchnorm:\n",
        "            model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Activation(activation))\n",
        "        if pool_type == \"Max\":\n",
        "            model.add(layers.MaxPooling2D())\n",
        "        else:\n",
        "            model.add(layers.AveragePooling2D())\n",
        "        if dropout and dropout > 0.0:\n",
        "            model.add(layers.Dropout(dropout))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    if dense_units and int(dense_units) > 0:\n",
        "        model.add(layers.Dense(int(dense_units), activation=None))\n",
        "        if batchnorm:\n",
        "            model.add(layers.BatchNormalization())\n",
        "        model.add(layers.Activation(activation))\n",
        "        if dropout and dropout > 0.0:\n",
        "            model.add(layers.Dropout(dropout))\n",
        "\n",
        "    model.add(layers.Dense(num_classes, activation=\"softmax\"))\n",
        "    model.compile(\n",
        "        optimizer=make_optimizer(optimizer_name, lr),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ========== Step 4: Training / Evaluation Utils ==========\n",
        "def train(model, x_train, y_train, x_val, y_val, epochs=5, batch_size=64, patience=3, verbose=1, seed=42):\n",
        "    set_seed(seed)\n",
        "    callbacks = []\n",
        "    if patience and patience > 0:\n",
        "        callbacks.append(tf.keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_accuracy\", patience=patience, restore_best_weights=True\n",
        "        ))\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=epochs, batch_size=batch_size, verbose=verbose,\n",
        "        callbacks=callbacks, shuffle=True\n",
        "    )\n",
        "    return history\n",
        "\n",
        "def evaluate(model, x_test, y_test):\n",
        "    return model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.figure()\n",
        "    plt.plot(history.history.get(\"accuracy\", []), label=\"train_acc\")\n",
        "    plt.plot(history.history.get(\"val_accuracy\", []), label=\"val_acc\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(); plt.title(\"Training vs Validation Accuracy\"); plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history.history.get(\"loss\", []), label=\"train_loss\")\n",
        "    plt.plot(history.history.get(\"val_loss\", []), label=\"val_loss\")\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "    plt.legend(); plt.title(\"Training vs Validation Loss\"); plt.show()\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    plt.imshow(cm, interpolation=\"nearest\"); plt.title(\"Confusion Matrix\"); plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45); plt.yticks(tick_marks, class_names)\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], \"d\"),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    plt.ylabel(\"True label\"); plt.xlabel(\"Predicted label\"); plt.tight_layout(); plt.show()\n",
        "\n",
        "def preview_samples(x, y_true, y_pred=None, class_names=None, n=8):\n",
        "    n = min(n, len(x)); cols = 4; rows = math.ceil(n/cols)\n",
        "    fig = plt.figure(figsize=(cols*2.5, rows*2.5))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(rows, cols, i+1)\n",
        "        img = x[i]\n",
        "        if img.shape[-1] == 1: plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "        else: plt.imshow(img)\n",
        "        title = \"\"\n",
        "        if y_true is not None:\n",
        "            yi = np.argmax(y_true[i]) if y_true.ndim > 1 else y_true[i]\n",
        "            title += f\"T:{class_names[yi] if class_names else yi}\"\n",
        "        if y_pred is not None:\n",
        "            yp = np.argmax(y_pred[i])\n",
        "            title += f\" | P:{class_names[yp] if class_names else yp}\"\n",
        "        plt.title(title); plt.axis(\"off\")\n",
        "    plt.tight_layout(); plt.show()\n",
        "\n",
        "# ========== Step 5: UI Widgets ==========\n",
        "dataset_dd = Dropdown(options=[\"MNIST\",\"Fashion-MNIST\",\"CIFAR-10\"], value=\"MNIST\", description=\"Dataset:\")\n",
        "model_tb = ToggleButtons(options=[\"MLP\",\"CNN\"], value=\"MLP\", description=\"Model:\")\n",
        "\n",
        "activation_dd = Dropdown(options=[\"relu\",\"tanh\",\"sigmoid\"], value=\"relu\", description=\"Activation:\")\n",
        "optimizer_dd  = Dropdown(options=[\"adam\",\"sgd\",\"rmsprop\"], value=\"adam\", description=\"Optimizer:\")\n",
        "lr_slider     = FloatSlider(value=0.001, min=1e-5, max=0.01, step=1e-4, description=\"LR:\", readout_format='.5f')\n",
        "epochs_slider = IntSlider(value=5, min=1, max=50, step=1, description=\"Epochs:\")\n",
        "batch_slider  = IntSlider(value=64, min=16, max=256, step=16, description=\"Batch:\")\n",
        "patience_slider = IntSlider(value=3, min=0, max=10, step=1, description=\"Patience:\")\n",
        "seed_slider   = IntSlider(value=42, min=0, max=9999, step=1, description=\"Seed:\")\n",
        "\n",
        "mlp_units_text = Text(value=\"128,64\", description=\"Hidden Units:\")\n",
        "mlp_dropout_slider = FloatSlider(value=0.0, min=0.0, max=0.7, step=0.05, description=\"Dropout:\")\n",
        "mlp_bn_chk = Checkbox(value=False, description=\"BatchNorm\")\n",
        "\n",
        "cnn_filters_text = Text(value=\"32,64\", description=\"Conv Filters:\")\n",
        "cnn_kernel_dd = Dropdown(options=[3,5], value=3, description=\"Kernel:\")\n",
        "cnn_pool_dd = Dropdown(options=[\"Max\",\"Average\"], value=\"Max\", description=\"Pooling:\")\n",
        "cnn_dense_units = IntText(value=128, description=\"Dense Units:\")\n",
        "cnn_dropout_slider = FloatSlider(value=0.25, min=0.0, max=0.7, step=0.05, description=\"Dropout:\")\n",
        "cnn_bn_chk = Checkbox(value=False, description=\"BatchNorm\")\n",
        "cnn_aug_chk = Checkbox(value=False, description=\"Augment\")\n",
        "\n",
        "load_btn = Button(description=\"1) Load Data\", button_style=\"info\")\n",
        "build_btn = Button(description=\"2) Build Model\", button_style=\"primary\")\n",
        "train_btn = Button(description=\"3) Train\", button_style=\"success\")\n",
        "eval_btn  = Button(description=\"4) Evaluate\", button_style=\"warning\")\n",
        "pred_btn  = Button(description=\"5) Predict Samples\")\n",
        "save_btn  = Button(description=\"\ud83d\udcbe Save Model\", button_style=\"info\")\n",
        "reset_btn = Button(description=\"Reset Outputs\", button_style=\"danger\")\n",
        "\n",
        "status_out = Output(layout=Layout(border=\"1px solid #ddd\", padding=\"6px\"))\n",
        "plot_out = Output(layout=Layout(border=\"1px solid #ddd\", padding=\"6px\"))\n",
        "\n",
        "# ========== Step 6: Handlers ==========\n",
        "def on_load_clicked(_):\n",
        "    with status_out:\n",
        "        clear_output(); print(\"Loading dataset:\", dataset_dd.value)\n",
        "    set_seed(seed_slider.value)\n",
        "    load_dataset(dataset_dd.value, val_size=0.1, seed=seed_slider.value)\n",
        "    with status_out:\n",
        "        print(f\"Train: {GLOBAL_STATE['x_train'].shape}, Val: {GLOBAL_STATE['x_val'].shape}, Test: {GLOBAL_STATE['x_test'].shape}\")\n",
        "        print(\"Classes:\", GLOBAL_STATE['class_names']); print(\"Input shape:\", GLOBAL_STATE['input_shape'])\n",
        "    with plot_out:\n",
        "        clear_output(); preview_samples(GLOBAL_STATE[\"x_train\"], GLOBAL_STATE[\"y_train\"], None, GLOBAL_STATE[\"class_names\"], n=8)\n",
        "\n",
        "def on_build_clicked(_):\n",
        "    with status_out: clear_output(); print(\"Building model:\", model_tb.value)\n",
        "    if model_tb.value == \"MLP\":\n",
        "        hidden_units = tuple(int(u.strip()) for u in mlp_units_text.value.split(\",\") if u.strip())\n",
        "        model = build_mlp(GLOBAL_STATE[\"input_shape\"], GLOBAL_STATE[\"num_classes\"],\n",
        "                          hidden_units=hidden_units, activation=activation_dd.value,\n",
        "                          dropout=mlp_dropout_slider.value, batchnorm=mlp_bn_chk.value,\n",
        "                          lr=lr_slider.value, optimizer_name=optimizer_dd.value)\n",
        "    else:\n",
        "        conv_filters = tuple(int(f.strip()) for f in cnn_filters_text.value.split(\",\") if f.strip())\n",
        "        model = build_cnn(GLOBAL_STATE[\"input_shape\"], GLOBAL_STATE[\"num_classes\"],\n",
        "                          conv_filters=conv_filters, kernel_size=cnn_kernel_dd.value,\n",
        "                          pool_type=cnn_pool_dd.value, dense_units=cnn_dense_units.value,\n",
        "                          activation=activation_dd.value, dropout=cnn_dropout_slider.value,\n",
        "                          batchnorm=cnn_bn_chk.value, lr=lr_slider.value,\n",
        "                          optimizer_name=optimizer_dd.value, augment=cnn_aug_chk.value)\n",
        "    GLOBAL_STATE[\"model\"] = model\n",
        "    with status_out: model.summary(print_fn=lambda x: print(x))\n",
        "\n",
        "def on_train_clicked(_):\n",
        "    with status_out: clear_output(); print(\"Training...\")\n",
        "    history = train(GLOBAL_STATE[\"model\"], GLOBAL_STATE[\"x_train\"], GLOBAL_STATE[\"y_train\"],\n",
        "                    GLOBAL_STATE[\"x_val\"], GLOBAL_STATE[\"y_val\"],\n",
        "                    epochs=epochs_slider.value, batch_size=batch_slider.value,\n",
        "                    patience=patience_slider.value, seed=seed_slider.value)\n",
        "    GLOBAL_STATE[\"history\"] = history\n",
        "    with status_out: print(\"Training complete.\")\n",
        "    with plot_out: plot_history(history)\n",
        "\n",
        "def on_eval_clicked(_):\n",
        "    with status_out: clear_output(); print(\"Evaluating on test set...\")\n",
        "    loss, acc = evaluate(GLOBAL_STATE[\"model\"], GLOBAL_STATE[\"x_test\"], GLOBAL_STATE[\"y_test\"])\n",
        "    with status_out: print(f\"Test Loss: {loss:.4f} | Test Accuracy: {acc:.4f}\")\n",
        "    y_true = np.argmax(GLOBAL_STATE[\"y_test\"], axis=1)\n",
        "    y_pred = np.argmax(GLOBAL_STATE[\"model\"].predict(GLOBAL_STATE[\"x_test\"], verbose=0), axis=1)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    with plot_out: plot_confusion_matrix(cm, GLOBAL_STATE[\"class_names\"])\n",
        "    with status_out: print(\"Classification Report:\\n\", classification_report(y_true, y_pred, target_names=GLOBAL_STATE[\"class_names\"]))\n",
        "\n",
        "def on_pred_clicked(_):\n",
        "    with status_out: clear_output(); print(\"Predicting a few samples...\")\n",
        "    idx = np.random.choice(len(GLOBAL_STATE[\"x_test\"]), size=8, replace=False)\n",
        "    x_s = GLOBAL_STATE[\"x_test\"][idx]; y_s = GLOBAL_STATE[\"y_test\"][idx]\n",
        "    y_pred = GLOBAL_STATE[\"model\"].predict(x_s, verbose=0)\n",
        "    with plot_out: preview_samples(x_s, y_s, y_pred, GLOBAL_STATE[\"class_names\"], n=len(idx))\n",
        "\n",
        "def on_save_clicked(_):\n",
        "    if GLOBAL_STATE[\"model\"] is None:\n",
        "        with status_out: print(\"\u26a0\ufe0f No model available to save. Train a model first.\")\n",
        "        return\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"saved_model_{timestamp}.h5\"\n",
        "    GLOBAL_STATE[\"model\"].save(filename)\n",
        "    with status_out: print(f\"\u2705 Model saved successfully as: {filename}\")\n",
        "\n",
        "def on_reset_clicked(_):\n",
        "    with status_out: clear_output(); print(\"Cleared outputs.\")\n",
        "    with plot_out: clear_output()\n",
        "\n",
        "load_btn.on_click(on_load_clicked)\n",
        "build_btn.on_click(on_build_clicked)\n",
        "train_btn.on_click(on_train_clicked)\n",
        "eval_btn.on_click(on_eval_clicked)\n",
        "pred_btn.on_click(on_pred_clicked)\n",
        "save_btn.on_click(on_save_clicked)\n",
        "reset_btn.on_click(on_reset_clicked)\n",
        "\n",
        "# ========== Step 7: Layout ==========\n",
        "mlp_box = VBox([Label(\"MLP Settings\"), HBox([mlp_units_text, mlp_dropout_slider, mlp_bn_chk])])\n",
        "cnn_box = VBox([Label(\"CNN Settings\"),\n",
        "                HBox([cnn_filters_text, cnn_kernel_dd, cnn_pool_dd]),\n",
        "                HBox([cnn_dense_units, cnn_dropout_slider, cnn_bn_chk, cnn_aug_chk])])\n",
        "shared_box = VBox([Label(\"Shared Hyperparameters\"),\n",
        "                   HBox([activation_dd, optimizer_dd]),\n",
        "                   HBox([lr_slider, epochs_slider, batch_slider, patience_slider, seed_slider])])\n",
        "\n",
        "top_row = HBox([dataset_dd, model_tb])\n",
        "ctrl_row = HBox([load_btn, build_btn, train_btn, eval_btn, pred_btn, save_btn, reset_btn])\n",
        "\n",
        "adv = Accordion([mlp_box, cnn_box, shared_box])\n",
        "adv.set_title(0, 'MLP'); adv.set_title(1, 'CNN'); adv.set_title(2, 'Training & Optimizer')\n",
        "\n",
        "ui = VBox([top_row, adv, ctrl_row, Label(\"Status / Logs\"), status_out, Label(\"Plots / Figures\"), plot_out])\n",
        "display(ui)\n"
      ]
    }
  ]
}